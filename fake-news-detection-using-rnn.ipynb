{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1037534,"sourceType":"datasetVersion","datasetId":572515}],"dockerImageVersionId":29985,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fake News Detection Using RNN \n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Tensorflow_logo.svg/115px-Tensorflow_logo.svg.png\"> \n\nThis notebook aims to classify fake news from real news using a recurrent neural network. To simplify the text preprocessing procedure, we will be using the built in functions from tensorflow instead of more established libraries like NLTK. \n\n**Here are the results: **\n\n* Accuracy on testing set: 0.9904231625835189\n* Precision on testing set: 0.9879573876794813\n* Recall on testing set: 0.9920930232558139\n\n*Free free to provide me with feedbacks.  \n\n**New update: confusion matrix is now expressed in terms of percentage rather than frequency.**\n\n\nImage Source: https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Tensorflow_logo.svg/115px-Tensorflow_logo.svg.png\nCode is on my Github: https://github.com/therealcyberlord\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport tensorflow as tf \nimport re \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nimport seaborn as sns \nplt.style.use('ggplot')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T15:44:44.352188Z","iopub.execute_input":"2021-12-06T15:44:44.352522Z","iopub.status.idle":"2021-12-06T15:44:49.157863Z","shell.execute_reply.started":"2021-12-06T15:44:44.35249Z","shell.execute_reply":"2021-12-06T15:44:49.157025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the data ","metadata":{}},{"cell_type":"code","source":"fake_df = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nreal_df = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-06T15:44:49.159927Z","iopub.execute_input":"2021-12-06T15:44:49.160432Z","iopub.status.idle":"2021-12-06T15:44:51.45956Z","shell.execute_reply.started":"2021-12-06T15:44:49.160378Z","shell.execute_reply":"2021-12-06T15:44:51.458783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for null values ","metadata":{}},{"cell_type":"code","source":"fake_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.462981Z","iopub.execute_input":"2021-12-06T15:44:51.463238Z","iopub.status.idle":"2021-12-06T15:44:51.483272Z","shell.execute_reply.started":"2021-12-06T15:44:51.463212Z","shell.execute_reply":"2021-12-06T15:44:51.482271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.484725Z","iopub.execute_input":"2021-12-06T15:44:51.485338Z","iopub.status.idle":"2021-12-06T15:44:51.504944Z","shell.execute_reply.started":"2021-12-06T15:44:51.485298Z","shell.execute_reply":"2021-12-06T15:44:51.503978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for unique values for subject. We want both data frames to have a similar distribution.","metadata":{}},{"cell_type":"code","source":"fake_df.subject.unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.509028Z","iopub.execute_input":"2021-12-06T15:44:51.509612Z","iopub.status.idle":"2021-12-06T15:44:51.519941Z","shell.execute_reply.started":"2021-12-06T15:44:51.509571Z","shell.execute_reply":"2021-12-06T15:44:51.518901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_df.subject.unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.523155Z","iopub.execute_input":"2021-12-06T15:44:51.523799Z","iopub.status.idle":"2021-12-06T15:44:51.531787Z","shell.execute_reply.started":"2021-12-06T15:44:51.52376Z","shell.execute_reply":"2021-12-06T15:44:51.530989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop the date from the dataset, I don't think there is a strong correlation between date and validity of the news. As we see above, subjects are not distributed evenly. We do not want that to influence the accuracy of our classifier. Therefore, we need to drop that as well. ","metadata":{}},{"cell_type":"code","source":"fake_df.drop(['date', 'subject'], axis=1, inplace=True)\nreal_df.drop(['date', 'subject'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.533212Z","iopub.execute_input":"2021-12-06T15:44:51.533698Z","iopub.status.idle":"2021-12-06T15:44:51.547417Z","shell.execute_reply.started":"2021-12-06T15:44:51.533662Z","shell.execute_reply":"2021-12-06T15:44:51.546728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 for fake news, and 1 for real news","metadata":{}},{"cell_type":"code","source":"fake_df['class'] = 0 \nreal_df['class'] = 1","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.550496Z","iopub.execute_input":"2021-12-06T15:44:51.550738Z","iopub.status.idle":"2021-12-06T15:44:51.555578Z","shell.execute_reply.started":"2021-12-06T15:44:51.550714Z","shell.execute_reply":"2021-12-06T15:44:51.554798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check out the distribution of fake news compare to real news","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df), color='orange')\nplt.bar('Real News', len(real_df), color='green')\nplt.title('Distribution of Fake News and Real News', size=15)\nplt.xlabel('News Type', size=15)\nplt.ylabel('# of News Articles', size=15)\n\n\ntotal_len = len(fake_df) + len(real_df)\nplt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df) / total_len, color='orange')\nplt.bar('Real News', len(real_df) / total_len, color='green')\nplt.title('Distribution of Fake News and Real News', size=15)\nplt.xlabel('News Type', size=15)\nplt.ylabel('Proportion of News Articles', size=15)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.557151Z","iopub.execute_input":"2021-12-06T15:44:51.557714Z","iopub.status.idle":"2021-12-06T15:44:51.808023Z","shell.execute_reply.started":"2021-12-06T15:44:51.557605Z","shell.execute_reply":"2021-12-06T15:44:51.807128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Difference in news articles:',len(fake_df)-len(real_df))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.809321Z","iopub.execute_input":"2021-12-06T15:44:51.809941Z","iopub.status.idle":"2021-12-06T15:44:51.81669Z","shell.execute_reply.started":"2021-12-06T15:44:51.809899Z","shell.execute_reply":"2021-12-06T15:44:51.815397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)\nnews_df","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.818305Z","iopub.execute_input":"2021-12-06T15:44:51.819006Z","iopub.status.idle":"2021-12-06T15:44:51.841204Z","shell.execute_reply.started":"2021-12-06T15:44:51.818964Z","shell.execute_reply":"2021-12-06T15:44:51.840368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combining the title with the text, it is much easier to process this way. ","metadata":{}},{"cell_type":"code","source":"news_df['text'] = news_df['title'] + news_df['text']\nnews_df.drop('title', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:51.842629Z","iopub.execute_input":"2021-12-06T15:44:51.843013Z","iopub.status.idle":"2021-12-06T15:44:52.000602Z","shell.execute_reply.started":"2021-12-06T15:44:51.842976Z","shell.execute_reply":"2021-12-06T15:44:51.99977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split into training and testing ","metadata":{}},{"cell_type":"code","source":"features = news_df['text']\ntargets = news_df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.20, random_state=18)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:52.002035Z","iopub.execute_input":"2021-12-06T15:44:52.002386Z","iopub.status.idle":"2021-12-06T15:44:52.01529Z","shell.execute_reply.started":"2021-12-06T15:44:52.00235Z","shell.execute_reply":"2021-12-06T15:44:52.014506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalizing our data: lower case, get rid of extra spaces, and url links. ","metadata":{}},{"cell_type":"code","source":"def normalize(data):\n    normalized = []\n    for i in data:\n        i = i.lower()\n        # get rid of urls\n        i = re.sub('https?://\\S+|www\\.\\S+', '', i)\n        # get rid of non words and extra spaces\n        i = re.sub('\\\\W', ' ', i)\n        i = re.sub('\\n', '', i)\n        i = re.sub(' +', ' ', i)\n        i = re.sub('^ ', '', i)\n        i = re.sub(' $', '', i)\n        normalized.append(i)\n    return normalized\n\nX_train = normalize(X_train)\nX_test = normalize(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:44:52.018141Z","iopub.execute_input":"2021-12-06T15:44:52.018508Z","iopub.status.idle":"2021-12-06T15:45:12.49293Z","shell.execute_reply.started":"2021-12-06T15:44:52.018474Z","shell.execute_reply":"2021-12-06T15:45:12.492034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_vocab = 10000\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:45:12.494218Z","iopub.execute_input":"2021-12-06T15:45:12.494586Z","iopub.status.idle":"2021-12-06T15:45:22.534183Z","shell.execute_reply.started":"2021-12-06T15:45:12.49455Z","shell.execute_reply":"2021-12-06T15:45:22.533276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert text to vectors, our classifier only takes numerical data. ","metadata":{}},{"cell_type":"code","source":"# tokenize the text into vectors \nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:45:22.535581Z","iopub.execute_input":"2021-12-06T15:45:22.535928Z","iopub.status.idle":"2021-12-06T15:45:33.756378Z","shell.execute_reply.started":"2021-12-06T15:45:22.535893Z","shell.execute_reply":"2021-12-06T15:45:33.755585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply padding so we have the same length for each article ","metadata":{}},{"cell_type":"code","source":"X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:45:33.757995Z","iopub.execute_input":"2021-12-06T15:45:33.758309Z","iopub.status.idle":"2021-12-06T15:45:35.90012Z","shell.execute_reply.started":"2021-12-06T15:45:33.758274Z","shell.execute_reply":"2021-12-06T15:45:35.899291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building the RNN.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_vocab, 128),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:45:35.901619Z","iopub.execute_input":"2021-12-06T15:45:35.901965Z","iopub.status.idle":"2021-12-06T15:45:39.10148Z","shell.execute_reply.started":"2021-12-06T15:45:35.901927Z","shell.execute_reply":"2021-12-06T15:45:39.100729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to use early stop, which stops when the validation loss no longer improve.","metadata":{}},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:45:39.10476Z","iopub.execute_input":"2021-12-06T15:45:39.105023Z","iopub.status.idle":"2021-12-06T15:49:40.730575Z","shell.execute_reply.started":"2021-12-06T15:45:39.104997Z","shell.execute_reply":"2021-12-06T15:49:40.726662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize our training over time ","metadata":{}},{"cell_type":"code","source":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Loss', size=20)\nplt.legend(prop={'size': 20})\nplt.show()\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Accuracy', size=20)\nplt.legend(prop={'size': 20})\nplt.ylim((0.5,1))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:40.742113Z","iopub.execute_input":"2021-12-06T15:49:40.744556Z","iopub.status.idle":"2021-12-06T15:49:41.096267Z","shell.execute_reply.started":"2021-12-06T15:49:40.74451Z","shell.execute_reply":"2021-12-06T15:49:41.095425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the testing set ","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:41.097619Z","iopub.execute_input":"2021-12-06T15:49:41.097967Z","iopub.status.idle":"2021-12-06T15:49:46.581968Z","shell.execute_reply.started":"2021-12-06T15:49:41.097929Z","shell.execute_reply":"2021-12-06T15:49:46.581272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0) ","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:46.584957Z","iopub.execute_input":"2021-12-06T15:49:46.585215Z","iopub.status.idle":"2021-12-06T15:49:52.723685Z","shell.execute_reply.started":"2021-12-06T15:49:46.585189Z","shell.execute_reply":"2021-12-06T15:49:52.722769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:52.725132Z","iopub.execute_input":"2021-12-06T15:49:52.725492Z","iopub.status.idle":"2021-12-06T15:49:52.768425Z","shell.execute_reply.started":"2021-12-06T15:49:52.725453Z","shell.execute_reply":"2021-12-06T15:49:52.767631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion matrix ","metadata":{}},{"cell_type":"code","source":"matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(16, 10))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=20)\nax.set_ylabel('True Labels', size=20)\nax.set_title('Confusion Matrix', size=20) \nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:52.769734Z","iopub.execute_input":"2021-12-06T15:49:52.77007Z","iopub.status.idle":"2021-12-06T15:49:52.982629Z","shell.execute_reply.started":"2021-12-06T15:49:52.770038Z","shell.execute_reply":"2021-12-06T15:49:52.981725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saves the weights for visualiation","metadata":{}},{"cell_type":"code","source":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:52.984347Z","iopub.execute_input":"2021-12-06T15:49:52.984746Z","iopub.status.idle":"2021-12-06T15:49:52.999228Z","shell.execute_reply.started":"2021-12-06T15:49:52.984705Z","shell.execute_reply":"2021-12-06T15:49:52.998321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = list(tokenizer.word_index.keys())\nword_index = word_index[:max_vocab-1]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:53.000911Z","iopub.execute_input":"2021-12-06T15:49:53.001275Z","iopub.status.idle":"2021-12-06T15:49:53.013936Z","shell.execute_reply.started":"2021-12-06T15:49:53.001238Z","shell.execute_reply":"2021-12-06T15:49:53.013087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write to file so we can use tensorflow's embedding projector to visualize what our network learned. This is only based on the fake news dataset. ","metadata":{}},{"cell_type":"code","source":"import io\n\nout_v = io.open('fakenews_vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('fakenews_meta.tsv', 'w', encoding='utf-8')\n\nfor num, word in enumerate(word_index):\n  vec = weights[num+1] # skip 0, it's padding.\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\nout_v.close()\nout_m.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T15:49:53.0162Z","iopub.execute_input":"2021-12-06T15:49:53.017031Z","iopub.status.idle":"2021-12-06T15:49:54.122814Z","shell.execute_reply.started":"2021-12-06T15:49:53.016991Z","shell.execute_reply":"2021-12-06T15:49:54.121953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embedding Projector: http://projector.tensorflow.org/\n<br>Picture credits: https://www.tensorflow.org/tensorboard/images/embedding_projector.png?raw=1\n\n![](https://www.tensorflow.org/tensorboard/images/embedding_projector.png?raw=1)","metadata":{}}]}